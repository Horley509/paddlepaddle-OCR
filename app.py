import streamlit as st
import os
import cv2
import json
import numpy as np
from paddleocr import PaddleOCR
import time

# --- Streamlit é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(page_title="å¤šç­–ç•¥OCRæ ‡æ³¨å·¥å…·", page_icon="ğŸ¤–", layout="wide")
st.title("å¤šç­–ç•¥OCRæ ‡æ³¨è¾…åŠ©å·¥å…· (Streamlitç‰ˆ)")
st.markdown("ä¸Šä¼ å›¾ç‰‡ï¼Œé€‰æ‹©ä¸åŒçš„OCRè¯­è¨€æ¨¡å‹å’Œå›¾åƒé¢„å¤„ç†ç­–ç•¥ï¼Œæ‰¾åˆ°æœ€ä½³çš„æ–‡æœ¬è¯†åˆ«ç»“æœã€‚")


# ==============================================================================
# --- æ¨¡å‹åŠ è½½ ---
@st.cache_resource
def load_ocr_models():
    print("--- æ­£åœ¨æ‰§è¡Œæ¨¡å‹åŠ è½½å¹¶ç¼“å­˜... ---")

    # ã€æœ€ç»ˆä¿®å¤ã€‘ç§»é™¤äº†æ— æ•ˆçš„ 'use_gpu': True å‚æ•°ã€‚
    # PaddlePaddle-GPU ç‰ˆæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨ GPUï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šã€‚
    common_params = {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}

    ocr_experts = {
        'english': PaddleOCR(lang='en', **common_params),
        'russian': PaddleOCR(lang='ru', **common_params),
        'arabic': PaddleOCR(lang='ar', **common_params),
        'korean': PaddleOCR(lang='korean', **common_params),
        'Spanish': PaddleOCR(lang='es', **common_params),
    }
    print("--- æ‰€æœ‰æ¨¡å‹å·²åŠ è½½å¹¶å­˜å…¥ç¼“å­˜ã€‚ ---")
    return ocr_experts


with st.spinner("é¦–æ¬¡å¯åŠ¨ï¼Œæ­£åœ¨åŠ è½½æ‰€æœ‰è¯­è¨€æ¨¡å‹ (åŠ è½½ä¸€æ¬¡åä¼šç¼“å­˜ï¼Œè¯·ç¨å€™)..."):
    ocr_experts = load_ocr_models()

if 'models_loaded' not in st.session_state:
    st.toast("æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¹¶å·²ç¼“å­˜ï¼", icon="âœ…")
    st.session_state.models_loaded = True


# --- é¢„å¤„ç†å‡½æ•° (æ— æ”¹åŠ¨) ---
def preprocess_clahe(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced_gray = clahe.apply(gray)
    return cv2.cvtColor(enhanced_gray, cv2.COLOR_GRAY2BGR)


def preprocess_sharpen(image):
    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])
    return cv2.filter2D(image, -1, kernel)


def preprocess_closing(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    kernel = np.ones((2, 2), np.uint8)
    closed_image = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)
    return cv2.cvtColor(closed_image, cv2.COLOR_GRAY2BGR)


# --- æ ¸å¿ƒå¤„ç†é€»è¾‘ (æ— æ”¹åŠ¨) ---
def process_image(original_image, selected_langs, selected_strategies, line_min_conf, line_min_len,
                  line_min_area_ratio):
    overall_best_result = {"results": [], "line_count": -1, "language_expert": "None", "preprocess_strategy": "None"}
    image_height, image_width, _ = original_image.shape
    total_area = image_width * image_height

    for lang_name in selected_langs:
        ocr_engine = ocr_experts[lang_name]
        pipelines = {
            "Original": original_image, "CLAHE": preprocess_clahe(original_image),
            "Sharpen": preprocess_sharpen(original_image), "Closing": preprocess_closing(original_image)
        }
        selected_pipelines = {name: func for name, func in pipelines.items() if name in selected_strategies}
        best_result_for_this_lang = {"results": [], "line_count": -1, "strategy": "None"}

        for strategy_name, img in selected_pipelines.items():
            ocr_results = ocr_engine.predict(img)
            high_quality_lines = []

            if ocr_results and ocr_results[0] is not None:
                results_dict = ocr_results[0]
                boxes = results_dict.get('dt_polys', [])
                texts = results_dict.get('rec_texts', [])
                scores = results_dict.get('rec_scores', [])

                for bbox, text, confidence in zip(boxes, texts, scores):
                    if confidence < line_min_conf or len(text.strip()) < line_min_len:
                        continue
                    if not isinstance(bbox, np.ndarray) and not isinstance(bbox, list): continue

                    box_np = np.array(bbox).astype(np.int32)
                    if (cv2.contourArea(box_np) / total_area) < line_min_area_ratio:
                        continue

                    high_quality_lines.append([bbox, (text, confidence)])

            line_count = len(high_quality_lines)
            if line_count > best_result_for_this_lang["line_count"]:
                best_result_for_this_lang = {"results": high_quality_lines, "line_count": line_count,
                                             "strategy": strategy_name}

        if best_result_for_this_lang['line_count'] > overall_best_result['line_count']:
            overall_best_result = {
                "results": best_result_for_this_lang['results'],
                "line_count": best_result_for_this_lang['line_count'],
                "language_expert": lang_name,
                "preprocess_strategy": best_result_for_this_lang['strategy']
            }

    visual_image = original_image.copy()
    output_data = []
    lang_label = "æœ€ä½³è¯­è¨€ä¸“å®¶" if len(selected_langs) > 1 else "é€‰ç”¨è¯­è¨€ä¸“å®¶"
    summary_text = f"**{lang_label}**: `{overall_best_result['language_expert']}`\n\n"
    summary_text += f"**æœ€ä½³é¢„å¤„ç†ç­–ç•¥**: `{overall_best_result['preprocess_strategy']}`\n\n"
    summary_text += f"**è¯†åˆ«å‡ºé«˜è´¨é‡æ–‡æœ¬è¡Œæ•°**: `{overall_best_result['line_count']}`"

    if overall_best_result['line_count'] > 0:
        for line_data in overall_best_result['results']:
            bbox, (text, confidence) = line_data
            box = np.array(bbox).astype(np.int32)
            cv2.polylines(visual_image, [box], isClosed=True, color=(0, 255, 0), thickness=2)
            min_x, min_y = int(min(p[0] for p in bbox)), int(min(p[1] for p in bbox))
            max_x, max_y = int(max(p[0] for p in bbox)), int(max(p[1] for p in bbox))
            simple_bbox = [[min_x, min_y], [max_x, max_y]]
            output_data.append({'bbox': simple_bbox, 'text': text})

    return visual_image, output_data, summary_text


# --- ç•Œé¢å¸ƒå±€ (æ— æ”¹åŠ¨) ---
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°é…ç½®")
    uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€å¼ æˆ–å¤šå¼ å›¾ç‰‡", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True)
    st.subheader("â€œå“æ§ä¸­å¿ƒâ€")
    line_min_conf = st.slider("æœ€ä½ç½®ä¿¡åº¦ (Confidence)", 0.0, 1.0, 0.4, 0.05)
    line_min_len = st.number_input("æœ€çŸ­æ–‡æœ¬é•¿åº¦", min_value=1, value=1)
    line_min_area_ratio = st.slider("æœ€å°æ–‡æœ¬åŒºåŸŸå æ¯”", 0.0, 0.01, 0.0001, 0.0001, format="%.4f")

    st.subheader("â€œå¤šè¯­è¨€ä¸“å®¶å§”å‘˜ä¼šâ€")
    detection_mode = st.radio("è¯­è¨€é€‰æ‹©æ¨¡å¼", ("è‡ªåŠ¨å¯»æ‰¾æœ€ä½³è¯­è¨€", "æ‰‹åŠ¨æŒ‡å®šè¯­è¨€"), key="detection_mode")
    available_langs = list(ocr_experts.keys())
    selected_langs = []

    if detection_mode == "æ‰‹åŠ¨æŒ‡å®šè¯­è¨€":
        manual_lang = st.selectbox("è¯·é€‰æ‹©ä¸€ä¸ªè¯­è¨€æ¨¡å‹", available_langs, index=0)
        selected_langs = [manual_lang]
    else:
        selected_langs = st.multiselect("é€‰æ‹©è¦å‚ä¸è¯„æµ‹çš„è¯­è¨€æ¨¡å‹", available_langs, default=available_langs)

    st.subheader("â€œé¢„å¤„ç†ä¸“å®¶å›¢é˜Ÿâ€")
    available_strategies = ["Original", "CLAHE", "Sharpen", "Closing"]
    selected_strategies = st.multiselect("é€‰æ‹©è¦å°è¯•çš„é¢„å¤„ç†ç­–ç•¥", available_strategies, default=available_strategies)

# --- ä¸»é¡µé¢ï¼šæ˜¾ç¤ºç»“æœ (æ— æ”¹åŠ¨) ---
if not uploaded_files:
    st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ å›¾ç‰‡å¼€å§‹å¤„ç†ã€‚")
elif not selected_langs:
    st.warning("è¯·åœ¨å·¦ä¾§é€‰æ‹©è‡³å°‘ä¸€ç§è¯­è¨€æ¨¡å‹ã€‚")
else:
    for uploaded_file in uploaded_files:
        st.header(f"å¤„ç†ç»“æœ: `{uploaded_file.name}`")
        bytes_data = uploaded_file.getvalue()
        file_bytes = np.frombuffer(bytes_data, np.uint8)
        original_image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)

        with st.spinner(f"æ­£åœ¨åˆ†æ `{uploaded_file.name}`... è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚"):
            visual_image, output_data, summary_text = process_image(
                original_image, selected_langs, selected_strategies, line_min_conf, line_min_len, line_min_area_ratio)
        st.success("å¤„ç†å®Œæˆï¼")

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("è¯Šæ–­æ€»ç»“")
            st.markdown(summary_text)
            st.subheader("è¯†åˆ«å‡ºçš„æ–‡æœ¬å†…å®¹")
            if output_data:
                for item in output_data:
                    st.text(f"- {item['text']}")
            else:
                st.warning("æœªèƒ½è¯†åˆ«å‡ºä»»ä½•ç¬¦åˆè´¨é‡è¦æ±‚çš„æ–‡æœ¬ã€‚")
        with col2:
            st.subheader("å¯è§†åŒ–ç»“æœ")
            st.image(cv2.cvtColor(visual_image, cv2.COLOR_BGR2RGB), caption="æ ‡æ³¨äº†æ–‡æœ¬æ¡†çš„å›¾ç‰‡", use_column_width=True)
        st.subheader("JSON ç»“æœä¸‹è½½")
        json_string = json.dumps(output_data, ensure_ascii=False, indent=4)
        st.json(json_string)
        st.download_button(label="ä¸‹è½½ JSON æ–‡ä»¶", file_name=f"{os.path.splitext(uploaded_file.name)[0]}.json",
                           mime="application/json", data=json_string)
        st.divider()